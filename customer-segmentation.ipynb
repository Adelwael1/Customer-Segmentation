{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "7c69b2163b1c7145f7a44d6d4c4e4d8a365e2ed1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!conda install -y orange3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from pandasql import sqldf\n",
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\", color_codes=True, font_scale=1.5)\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "py.offline.init_notebook_mode()\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, norm, probplot, boxcox\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import Orange\n",
    "from Orange.data import Domain, DiscreteVariable, ContinuousVariable\n",
    "from orangecontrib.associate.fpgrowth import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "62aeeae5297209577312f1a5c3d42df6c795e630"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bbc72959338d63b729bd67110024cee7f5a5b704",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cs_df = pd.read_excel(io=r'../input/Online Retail.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6991e4853eb086acefec0cd3f10e48b7326fa3b2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def rstr(df, pred=None): \n",
    "    obs = df.shape[0]\n",
    "    types = df.dtypes\n",
    "    counts = df.apply(lambda x: x.count())\n",
    "    uniques = df.apply(lambda x: [x.unique()])\n",
    "    nulls = df.apply(lambda x: x.isnull().sum())\n",
    "    distincts = df.apply(lambda x: x.unique().shape[0])\n",
    "    missing_ration = (df.isnull().sum()/ obs) * 100\n",
    "    skewness = df.skew()\n",
    "    kurtosis = df.kurt() \n",
    "    print('Data shape:', df.shape)\n",
    "    \n",
    "    if pred is None:\n",
    "        cols = ['types', 'counts', 'distincts', 'nulls', 'missing ration', 'uniques', 'skewness', 'kurtosis']\n",
    "        str = pd.concat([types, counts, distincts, nulls, missing_ration, uniques, skewness, kurtosis], axis = 1, sort=True)\n",
    "\n",
    "    else:\n",
    "        corr = df.corr()[pred]\n",
    "        str = pd.concat([types, counts, distincts, nulls, missing_ration, uniques, skewness, kurtosis, corr], axis = 1, sort=True)\n",
    "        corr_col = 'corr '  + pred\n",
    "        cols = ['types', 'counts', 'distincts', 'nulls', 'missing ration', 'uniques', 'skewness', 'kurtosis', corr_col ]\n",
    "    \n",
    "    str.columns = cols\n",
    "    dtypes = str.types.value_counts()\n",
    "    print('___________________________\\nData types:\\n',str.types.value_counts())\n",
    "    print('___________________________')\n",
    "    return str\n",
    "\n",
    "details = rstr(cs_df)\n",
    "display(details.sort_values(by='missing ration', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7afee98474b0d0ea86b505e76a48932bf30741e4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cs_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4fe748a43f47869e72cb3d36c4e4eca764925d34"
   },
   "source": [
    "We can observe from the preceding output that Quantity and UnitPrice are having negative values, which may mean that we may have some return transactions in our data also. \n",
    "As our goal is customer segmentation and market basket analysis, it is important that these records are removed, but first we will take a look at whether there are records where both are negative or if one of them is negative and the other is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9452280ec8d24457bd8fb8ec253fec2151fe81d0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print('Check if we had negative quantity and prices at same register:',\n",
    "     'No' if cs_df[(cs_df.Quantity<0) & (cs_df.UnitPrice<0)].shape[0] == 0 else 'Yes', '\\n')\n",
    "print('Check how many register we have where quantity is negative',\n",
    "      'and prices is 0 or vice-versa:',\n",
    "      cs_df[(cs_df.Quantity<=0) & (cs_df.UnitPrice<=0)].shape[0])\n",
    "print('\\nWhat is the customer ID of the registers above:',\n",
    "      cs_df.loc[(cs_df.Quantity<=0) & (cs_df.UnitPrice<=0), \n",
    "                ['CustomerID']].CustomerID.unique())\n",
    "print('\\n% Negative Quantity: {:3.2%}'.format(cs_df[(cs_df.Quantity<0)].shape[0]/cs_df.shape[0]))\n",
    "print('\\nAll register with negative quantity has Invoice start with:', \n",
    "      cs_df.loc[(cs_df.Quantity<0) & ~(cs_df.CustomerID.isnull()), 'InvoiceNo'].apply(lambda x: x[0]).unique())\n",
    "print('\\nSee an example of negative quantity and others related records:')\n",
    "display(cs_df[(cs_df.CustomerID==12472) & (cs_df.StockCode==22244)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "64fb89e185fe217a5ea5ef13103a9b84c1d0e4dd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print('Check register with UnitPrice negative:')\n",
    "display(cs_df[(cs_df.UnitPrice<0)])\n",
    "print(\"Sales records with Customer ID and zero in Unit Price:\",cs_df[(cs_df.UnitPrice==0)  & ~(cs_df.CustomerID.isnull())].shape[0])\n",
    "cs_df[(cs_df.UnitPrice==0)  & ~(cs_df.CustomerID.isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f028a6e138303c18f2b34c9923ca338d806ae282"
   },
   "source": [
    "As you can see, there are no records where quantity and price are negative, but there are 1.336 records where one of them is and the other is 0. However, note that for all these records we do not have the customer ID. So we conclude that we can erase all records in that quantity or the price and negative. In addition, by the foregoing summary we see that there are 135,080 records without customer identification that we may also disregard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "35c50ac6f5bf70236029b9bdcc680cd48e72b434",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Remove register withou CustomerID\n",
    "cs_df = cs_df[~(cs_df.CustomerID.isnull())]\n",
    "\n",
    "# Remove negative or return transactions\n",
    "cs_df = cs_df[~(cs_df.Quantity<0)]\n",
    "cs_df = cs_df[cs_df.UnitPrice>0]\n",
    "\n",
    "details = rstr(cs_df)\n",
    "display(details.sort_values(by='distincts', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d207a3621ee6566f4cd43e3b5dbf17b65d2587f1"
   },
   "source": [
    "\n",
    "After this first cleanup, note that we still have more description than inventory codes, so we still have some inconsistency on the basis that requires further investigation. LetÂ´s see it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec0f3284d5a7f13a24f53414a4ce002504b164c1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cat_des_df = cs_df.groupby([\"StockCode\",\"Description\"]).count().reset_index()\n",
    "display(cat_des_df.StockCode.value_counts()[cat_des_df.StockCode.value_counts()>1].reset_index().head())\n",
    "cs_df[cs_df['StockCode'] == cat_des_df.StockCode.value_counts()[cat_des_df.StockCode.value_counts()>1]\n",
    "      .reset_index()['index'][4]]['Description'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e998eb5284ffd1d392f611a8f514a2ff7534231e"
   },
   "source": [
    "This gives the multiple descriptions for one of those items and we witness the simple ways in which data quality can be corrupted in any dataset. A simple spelling mistake can end up in reducing data quality and an erroneous analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "650eaa099f31c4f3ca031ca2b542a27dcaef7964",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "unique_desc = cs_df[[\"StockCode\", \"Description\"]].groupby(by=[\"StockCode\"]).\\\n",
    "                apply(pd.DataFrame.mode).reset_index(drop=True)\n",
    "q = '''\n",
    "select df.InvoiceNo, df.StockCode, un.Description, df.Quantity, df.InvoiceDate,\n",
    "       df.UnitPrice, df.CustomerID, df.Country\n",
    "from cs_df as df INNER JOIN \n",
    "     unique_desc as un on df.StockCode = un.StockCode\n",
    "'''\n",
    "\n",
    "cs_df = pysqldf(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aeaa8b828084eb8eefc208f8e1d5067d0a21f318",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cs_df.InvoiceDate = pd.to_datetime(cs_df.InvoiceDate)\n",
    "cs_df['amount'] = cs_df.Quantity*cs_df.UnitPrice\n",
    "cs_df.CustomerID = cs_df.CustomerID.astype('Int64')\n",
    "\n",
    "details = rstr(cs_df)\n",
    "display(details.sort_values(by='distincts', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d637b0d255123526b0df1172c7a433bd4aab7190",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 7))\n",
    "f1 = fig.add_subplot(121)\n",
    "g = cs_df.groupby([\"Country\"]).amount.sum().sort_values(ascending = False).plot(kind='bar', title='Amount Sales by Country')\n",
    "cs_df['Internal'] = cs_df.Country.apply(lambda x: 'Yes' if x=='United Kingdom' else 'No' )\n",
    "f2 = fig.add_subplot(122)\n",
    "market = cs_df.groupby([\"Internal\"]).amount.sum().sort_values(ascending = False)\n",
    "g = plt.pie(market, labels=market.index, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.title('Internal Market')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6470dea1531ac0ff12cf3dc693767a684eb3c6d3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 7))\n",
    "PercentSales =  np.round((cs_df.groupby([\"CustomerID\"]).amount.sum().\\\n",
    "                          sort_values(ascending = False)[:51].sum()/cs_df.groupby([\"CustomerID\"]).\\\n",
    "                          amount.sum().sort_values(ascending = False).sum()) * 100, 2)\n",
    "g = cs_df.groupby([\"CustomerID\"]).amount.sum().sort_values(ascending = False)[:51].\\\n",
    "    plot(kind='bar', title='Top Customers: {:3.2f}% Sales Amount'.format(PercentSales))\n",
    "\n",
    "fig = plt.figure(figsize=(25, 7))\n",
    "f1 = fig.add_subplot(121)\n",
    "PercentSales =  np.round((cs_df.groupby([\"CustomerID\"]).amount.sum().\\\n",
    "                          sort_values(ascending = False)[:10].sum()/cs_df.groupby([\"CustomerID\"]).\\\n",
    "                          amount.sum().sort_values(ascending = False).sum()) * 100, 2)\n",
    "g = cs_df.groupby([\"CustomerID\"]).amount.sum().sort_values(ascending = False)[:10]\\\n",
    "    .plot(kind='bar', title='Top 10 Customers: {:3.2f}% Sales Amont'.format(PercentSales))\n",
    "f1 = fig.add_subplot(122)\n",
    "PercentSales =  np.round((cs_df.groupby([\"CustomerID\"]).amount.count().\\\n",
    "                          sort_values(ascending = False)[:10].sum()/cs_df.groupby([\"CustomerID\"]).\\\n",
    "                          amount.count().sort_values(ascending = False).sum()) * 100, 2)\n",
    "g = cs_df.groupby([\"CustomerID\"]).amount.count().sort_values(ascending = False)[:10].\\\n",
    "    plot(kind='bar', title='Top 10 Customers: {:3.2f}% Event Sales'.format(PercentSales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc1dc29ee91fe1f34ac131c9c733bdc9b883c12e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "AmoutSum = cs_df.groupby([\"Description\"]).amount.sum().sort_values(ascending = False)\n",
    "inv = cs_df[[\"Description\", \"InvoiceNo\"]].groupby([\"Description\"]).InvoiceNo.unique().\\\n",
    "      agg(np.size).sort_values(ascending = False)\n",
    "\n",
    "fig = plt.figure(figsize=(25, 7))\n",
    "f1 = fig.add_subplot(121)\n",
    "Top10 = list(AmoutSum[:10].index)\n",
    "PercentSales =  np.round((AmoutSum[Top10].sum()/AmoutSum.sum()) * 100, 2)\n",
    "PercentEvents = np.round((inv[Top10].sum()/inv.sum()) * 100, 2)\n",
    "g = AmoutSum[Top10].\\\n",
    "    plot(kind='bar', title='Top 10 Products in Sales Amount: {:3.2f}% of Amount and {:3.2f}% of Events'.\\\n",
    "                       format(PercentSales, PercentEvents))\n",
    "\n",
    "f1 = fig.add_subplot(122)\n",
    "Top10Ev = list(inv[:10].index)\n",
    "PercentSales =  np.round((AmoutSum[Top10Ev].sum()/AmoutSum.sum()) * 100, 2)\n",
    "PercentEvents = np.round((inv[Top10Ev].sum()/inv.sum()) * 100, 2)\n",
    "g = inv[Top10Ev].\\\n",
    "    plot(kind='bar', title='Events of top 10 most sold products: {:3.2f}% of Amount and {:3.2f}% of Events'.\\\n",
    "                       format(PercentSales, PercentEvents))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(25, 7))\n",
    "Top15ev = list(inv[:15].index)\n",
    "PercentSales =  np.round((AmoutSum[Top15ev].sum()/AmoutSum.sum()) * 100, 2)\n",
    "PercentEvents = np.round((inv[Top15ev].sum()/inv.sum()) * 100, 2)\n",
    "g = AmoutSum[Top15ev].sort_values(ascending = False).\\\n",
    "    plot(kind='bar', \n",
    "         title='Sales Amount of top 15 most sold products: {:3.2f}% of Amount and {:3.2f}% of Events'.\\\n",
    "         format(PercentSales, PercentEvents))\n",
    "\n",
    "fig = plt.figure(figsize=(25, 7))\n",
    "Top50 = list(AmoutSum[:50].index)\n",
    "PercentSales =  np.round((AmoutSum[Top50].sum()/AmoutSum.sum()) * 100, 2)\n",
    "PercentEvents = np.round((inv[Top50].sum()/inv.sum()) * 100, 2)\n",
    "g = AmoutSum[Top50].\\\n",
    "    plot(kind='bar', \n",
    "         title='Top 50 Products in Sales Amount: {:3.2f}% of Amount and {:3.2f}% of Events'.\\\n",
    "         format(PercentSales, PercentEvents))\n",
    "\n",
    "fig = plt.figure(figsize=(25, 7))\n",
    "Top50Ev = list(inv[:50].index)\n",
    "PercentSales =  np.round((AmoutSum[Top50Ev].sum()/AmoutSum.sum()) * 100, 2)\n",
    "PercentEvents = np.round((inv[Top50Ev].sum()/inv.sum()) * 100, 2)\n",
    "g = inv[Top50Ev].\\\n",
    "    plot(kind='bar', title='Top 50 most sold products: {:3.2f}% of Amount and {:3.2f}% of Events'.\\\n",
    "                       format(PercentSales, PercentEvents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53c480ec2a5060dd8ad567ff38403cd23d366b7d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "refrence_date = cs_df.InvoiceDate.max() + datetime.timedelta(days = 1)\n",
    "print('Reference Date:', refrence_date)\n",
    "cs_df['days_since_last_purchase'] = (refrence_date - cs_df.InvoiceDate).astype('timedelta64[D]')\n",
    "customer_history_df =  cs_df[['CustomerID', 'days_since_last_purchase']].groupby(\"CustomerID\").min().reset_index()\n",
    "customer_history_df.rename(columns={'days_since_last_purchase':'recency'}, inplace=True)\n",
    "customer_history_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "58fd0dce757a660abc0ffcc4b2379131fd7795df"
   },
   "source": [
    "We will plot the Recency Distribution and QQ-plot to identify substantive departures from normality, likes outliers, skewness and kurtosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "016a1b847295408b2674270be4bfb762e218845d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def QQ_plot(data, measure):\n",
    "    fig = plt.figure(figsize=(20,7))\n",
    "\n",
    "    #Get the fitted parameters used by the function\n",
    "    (mu, sigma) = norm.fit(data)\n",
    "\n",
    "    #Kernel Density plot\n",
    "    fig1 = fig.add_subplot(121)\n",
    "    sns.distplot(data, fit=norm)\n",
    "    fig1.set_title(measure + ' Distribution ( mu = {:.2f} and sigma = {:.2f} )'.format(mu, sigma), loc='center')\n",
    "    fig1.set_xlabel(measure)\n",
    "    fig1.set_ylabel('Frequency')\n",
    "\n",
    "    #QQ plot\n",
    "    fig2 = fig.add_subplot(122)\n",
    "    res = probplot(data, plot=fig2)\n",
    "    fig2.set_title(measure + ' Probability Plot (skewness: {:.6f} and kurtosis: {:.6f} )'.format(data.skew(), data.kurt()), loc='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "QQ_plot(customer_history_df.recency, 'Recency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "770e3c46ac1af0ff09975395d51471e6cbcb4dd2"
   },
   "source": [
    "#### Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c9a5c746d61644b9ae7601b803a94372dff57aa2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "customer_freq = (cs_df[['CustomerID', 'InvoiceNo']].groupby([\"CustomerID\", 'InvoiceNo']).count().reset_index()).\\\n",
    "                groupby([\"CustomerID\"]).count().reset_index()\n",
    "customer_freq.rename(columns={'InvoiceNo':'frequency'},inplace=True)\n",
    "customer_history_df = customer_history_df.merge(customer_freq)\n",
    "QQ_plot(customer_history_df.frequency, 'Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "669115b1715b0b7f04c885d5def179018219b6df",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "customer_monetary_val = cs_df[['CustomerID', 'amount']].groupby(\"CustomerID\").sum().reset_index()\n",
    "customer_history_df = customer_history_df.merge(customer_monetary_val)\n",
    "QQ_plot(customer_history_df.amount, 'Amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20d4ef2c2a802c44750a9ba7992e47a95aa8f401",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "customer_history_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1abc3ef9ca15c3fcb5a43904b6b339945ac5b911"
   },
   "source": [
    "### Data Preprocessing\n",
    "Once we have created our customer value dataset, we will perform some preprocessing on the data. For our clustering, we will be using the K-means clustering algorithm. One of the requirements for proper functioning of the algorithm is the mean centering of the variable values. Mean centering of a variable value means that we will replace the actual value of the variable with a standardized value, so that the variable has a mean of 0 and variance of 1. This ensures that all the variables are in the same range and the difference in ranges of values doesn't cause the algorithm to not perform well. This is akin to feature scaling.\n",
    "\n",
    "Another problem that you can investigate about is the huge range of values each variable can take. This\n",
    "problem is particularly noticeable for the monetary amount variable. To take care of this problem, we will transform all the variables on the log scale. This transformation, along with the standardization, will ensure that the input to our algorithm is a homogenous set of scaled and transformed values.\n",
    "\n",
    "An important point about the data preprocessing step is that sometimes we need it to be reversible. In our case, we will have the clustering results in terms of the log transformed and scaled variable. But to make inferences in terms of the original data, we will need to reverse transform all the variable so that we get back the actual RFM figures. This can be done by using the preprocessing capabilities of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dee7fc51a02a1003cc97fd2d94b30b59f50d3dd7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "customer_history_df['recency_log'] = customer_history_df['recency'].apply(math.log)\n",
    "customer_history_df['frequency_log'] = customer_history_df['frequency'].apply(math.log)\n",
    "customer_history_df['amount_log'] = customer_history_df['amount'].apply(math.log)\n",
    "feature_vector = ['amount_log', 'recency_log','frequency_log']\n",
    "X_subset = customer_history_df[feature_vector] #.as_matrix()\n",
    "scaler = preprocessing.StandardScaler().fit(X_subset)\n",
    "X_scaled = scaler.transform(X_subset)\n",
    "pd.DataFrame(X_scaled, columns=X_subset.columns).describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e696e6da1a07c593f549931e24ee666f2e1e80c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,14))\n",
    "f1 = fig.add_subplot(221); sns.regplot(x='recency', y='amount', data=customer_history_df)\n",
    "f1 = fig.add_subplot(222); sns.regplot(x='frequency', y='amount', data=customer_history_df)\n",
    "f1 = fig.add_subplot(223); sns.regplot(x='recency_log', y='amount_log', data=customer_history_df)\n",
    "f1 = fig.add_subplot(224); sns.regplot(x='frequency_log', y='amount_log', data=customer_history_df)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "xs =customer_history_df.recency_log\n",
    "ys = customer_history_df.frequency_log\n",
    "zs = customer_history_df.amount_log\n",
    "ax.scatter(xs, ys, zs, s=5)\n",
    "\n",
    "ax.set_xlabel('Recency')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_zlabel('Monetary')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5c7b32efb9db750a7f1b6599feaa5d84c3bcbe30"
   },
   "source": [
    "The obvious patterns we can see from the plots above is that costumers who buy with a higher frequency and more recency tend to spend more based on the increasing trend in Monetary (amount value) with a corresponding increasing and decreasing trend for Frequency and Recency, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5fd3d3cc44bba2130e1d315dd8841afbc8a18783",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cl = 50\n",
    "corte = 0.1\n",
    "\n",
    "anterior = 100000000000000\n",
    "cost = [] \n",
    "K_best = cl\n",
    "\n",
    "for k in range (1, cl+1):\n",
    "    # Create a kmeans model on our data, using k clusters.  random_state helps ensure that the algorithm returns the same results each time.\n",
    "    model = KMeans(\n",
    "        n_clusters=k, \n",
    "        init='k-means++', #'random',\n",
    "        n_init=10,\n",
    "        max_iter=300,\n",
    "        tol=1e-04,\n",
    "        random_state=101)\n",
    "\n",
    "    model = model.fit(X_scaled)\n",
    "\n",
    "    # These are our fitted labels for clusters -- the first cluster has label 0, and the second has label 1.\n",
    "    labels = model.labels_\n",
    " \n",
    "    # Sum of distances of samples to their closest cluster center\n",
    "    interia = model.inertia_\n",
    "    if (K_best == cl) and (((anterior - interia)/anterior) < corte): K_best = k - 1\n",
    "    cost.append(interia)\n",
    "    anterior = interia\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(range (1, cl+1), cost, c='red')\n",
    "plt.show()\n",
    "\n",
    "# Create a kmeans model with the best K.\n",
    "print('The best K sugest: ',K_best)\n",
    "model = KMeans(n_clusters=K_best, init='k-means++', n_init=10,max_iter=300, tol=1e-04, random_state=101)\n",
    "\n",
    "# Note I'm scaling the data to normalize it! Important for good results.\n",
    "model = model.fit(X_scaled)\n",
    "\n",
    "# These are our fitted labels for clusters -- the first cluster has label 0, and the second has label 1.\n",
    "labels = model.labels_\n",
    "\n",
    "# And we'll visualize it:\n",
    "#plt.scatter(X_scaled[:,0], X_scaled[:,1], c=model.labels_.astype(float))\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "ax = fig.add_subplot(121)\n",
    "plt.scatter(x = X_scaled[:,1], y = X_scaled[:,0], c=model.labels_.astype(float))\n",
    "ax.set_xlabel(feature_vector[1])\n",
    "ax.set_ylabel(feature_vector[0])\n",
    "ax = fig.add_subplot(122)\n",
    "plt.scatter(x = X_scaled[:,2], y = X_scaled[:,0], c=model.labels_.astype(float))\n",
    "ax.set_xlabel(feature_vector[2])\n",
    "ax.set_ylabel(feature_vector[0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4ba97c060c7d5f846ea39bf6a2aa19fac4f5f4d"
   },
   "source": [
    "Note that by the Elbow method from a K equal to 3 we already observed low rates of gain in the decay of the distortions with the decrease of K reaching the limit of 10% with the K equal to 7. With this in mind, we will begin to evaluate the options more deeply with 3, and 7, starting with the silhouette analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff8a9bb48372e0ca3426a4f3446fb109eadc14ff",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cluster_centers = dict()\n",
    "\n",
    "for n_clusters in range(3,K_best+1,2):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "    fig.set_size_inches(25, 7)\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_ylim([0, len(X_scaled) + (n_clusters + 1) * 10])\n",
    "\n",
    "    clusterer = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10,max_iter=300, tol=1e-04, random_state=101)\n",
    "    cluster_labels = clusterer.fit_predict(X_scaled)\n",
    "\n",
    "    silhouette_avg = silhouette_score(X = X_scaled, labels = cluster_labels)\n",
    "    cluster_centers.update({n_clusters :{'cluster_center':clusterer.cluster_centers_,\n",
    "                                         'silhouette_score':silhouette_avg,\n",
    "                                         'labels':cluster_labels}\n",
    "                           })\n",
    "\n",
    "    sample_silhouette_values = silhouette_samples(X = X_scaled, labels = cluster_labels)\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.Spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xticks([-0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "    colors = cm.Spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    \n",
    "    centers = clusterer.cluster_centers_\n",
    "    y = 0\n",
    "    x = 1\n",
    "    ax2.scatter(X_scaled[:, x], X_scaled[:, y], marker='.', s=30, lw=0, alpha=0.7, c=colors, edgecolor='k')   \n",
    "    ax2.scatter(centers[:, x], centers[:, y], marker='o', c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[x], c[y], marker='$%d$' % i, alpha=1, s=50, edgecolor='k')\n",
    "    ax2.set_title(\"{} Clustered data\".format(n_clusters))\n",
    "    ax2.set_xlabel(feature_vector[x])\n",
    "    ax2.set_ylabel(feature_vector[y])\n",
    "\n",
    "    x = 2\n",
    "    ax3.scatter(X_scaled[:, x], X_scaled[:, y], marker='.', s=30, lw=0, alpha=0.7, c=colors, edgecolor='k')   \n",
    "    ax3.scatter(centers[:, x], centers[:, y], marker='o', c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "    for i, c in enumerate(centers):\n",
    "        ax3.scatter(c[x], c[y], marker='$%d$' % i, alpha=1, s=50, edgecolor='k')\n",
    "    ax3.set_title(\"Silhouette score: {:1.2f}\".format(cluster_centers[n_clusters]['silhouette_score']))\n",
    "    ax3.set_xlabel(feature_vector[x])\n",
    "    ax3.set_ylabel(feature_vector[y])\n",
    "    \n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b76cf9fee9d15085fbbb79d37f58a9d1a6719cac"
   },
   "source": [
    "When we look at the results of the clustering process, we can infer some interesting insights:\n",
    "\n",
    "- First notice that all K clusters options is valid, because they don't have presence of clusters with below average silhouette scores. \n",
    "- In the other hand, all options had a some wide fluctuations in the size of the silhouette plots. \n",
    "\n",
    "So, the best choice may lie on the option that gives us a simpler business explanation and at the same time target customers in focus groups with sizes closer to the desired. \n",
    "\n",
    "#### Clusters Center:\n",
    "Let's look at the cluster center values after returning them to normal values from the log and scaled version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1d2714f82874a1e8f49f706c2b0c3574d8e6092",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "features = ['amount',  'recency',  'frequency']\n",
    "for i in range(3,K_best+1,2):\n",
    "    print(\"for {} clusters the silhouette score is {:1.2f}\".format(i, cluster_centers[i]['silhouette_score']))\n",
    "    print(\"Centers of each cluster:\")\n",
    "    cent_transformed = scaler.inverse_transform(cluster_centers[i]['cluster_center'])\n",
    "    print(pd.DataFrame(np.exp(cent_transformed),columns=features))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6ad52ac233e70864582390bb0b271ba7232dfe2"
   },
   "source": [
    "#### Clusters Insights:\n",
    "\n",
    "With the plots and the center in the correct units, let's see some insights by each clusters groups:\n",
    "\n",
    "***In the three-cluster:***\n",
    "- The tree clusters appears have a good stark differences in the Monetary value of the customer, we will confirm this by a box plot.\n",
    "- Cluster 1 is the cluster of high value customer who shops frequently and is certainly an important segment for each business.\n",
    "- In the similar way we obtain customer groups with low and medium spends in clusters with labels 0 and 2, respectively.\n",
    "- Frequency and Recency correlate perfectly to the Monetary value based on the trend (High Monetary-Low Recency-High Frequency).\n",
    "\n",
    "***In the five-cluster:***\n",
    "- Note that clusters 0 and 1 are very similar to their cluster in the configuration with only 3 clusters.\n",
    "- The cluster 1 appears more robust on the affirmation of those who shop often and with high amount.\n",
    "- The cluster 2 are those who have a decent spend but are not as frequent as the cluster 1\n",
    "- The cluster 4 purchases medium amounts, with a relatively low frequency and not very recent\n",
    "- The cluster 3 makes low-cost purchases, with a relatively low frequency, but above 1, and made their last purchase more recently. This group of customers probably response to price discounts and can be subject to loyalty promotions to try increase the medium-ticket, strategy that can be better defined when we analyzing the market basket. \n",
    "- The silhouette score matrix says that the  five cluster segments are less optimal then the three cluster segments. \n",
    "\n",
    "***In the five-cluster:***\n",
    "- Definitely cluster 6 defines those who shop often and with high amount.\n",
    "- Clusters 1 and 5 show good spending and good frequency, only deferring in how recent were their last purchases, where 5 is older, which suggests an active action to sell to group 5 as soon as possible and another to 1 seeking to raise its frequency.\n",
    "- Cluster 0 presents the fourth best purchase and a reasonable frequency, but this is a long time without buying. This group should be sensible to promotions and activations, so that they do not get lost and make their next purchase.\n",
    "- Cluster 5 is similar to 0, but has made its purchases more recently and has a slightly better periodicity. Then actions must be taken to raise their frequency and reduce the chances of them migrating to cluster 0 by staying longer without purchasing products.\n",
    "\n",
    "#### Drill Down Clusters:\n",
    "\n",
    "To further drill down on this point and find out the quality of these difference, we can label our data with the corresponding cluster label and then visualize these differences. The following code will extract the clustering label and attach it with our customer summary dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e7d3215df7d048b065710ccac3f3ce75e97b77f6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "customer_history_df['clusters_3'] = cluster_centers[3]['labels'] \n",
    "customer_history_df['clusters_5'] = cluster_centers[5]['labels']\n",
    "customer_history_df['clusters_7'] = cluster_centers[7]['labels']\n",
    "display(customer_history_df.head())\n",
    "\n",
    "fig = plt.figure(figsize=(20,7))\n",
    "f1 = fig.add_subplot(131)\n",
    "market = customer_history_df.clusters_3.value_counts()\n",
    "g = plt.pie(market, labels=market.index, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.title('3 Clusters')\n",
    "f1 = fig.add_subplot(132)\n",
    "market = customer_history_df.clusters_5.value_counts()\n",
    "g = plt.pie(market, labels=market.index, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.title('5 Clusters')\n",
    "f1 = fig.add_subplot(133)\n",
    "market = customer_history_df.clusters_7.value_counts()\n",
    "g = plt.pie(market, labels=market.index, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.title('7 Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c8fde61cc65e4b8b5564a43bfbc37f29b3810619"
   },
   "source": [
    "Once we have the labels assigned to each of the customers, our task is simple. Now we want to find out how the summary of customer in each group is varying. If we can visualize that information we will able to find out the differences in the clusters of customers and we can modify our strategy on the basis of those differences.\n",
    "\n",
    "The following code leverages plotly and will take the cluster labels we got for each configurations clusters and create boxplots. Plotly enables us to interact with the plots to see the central tendency values in each boxplot in the notebook. Note that we want to avoid the extremely high outlier values of each group, as they will interfere in making a good observation around the central tendencies of each cluster. Since we have only positive values, we will restrict the data such that only data points which are less than 0.95th percentile of the cluster is used. This will give us good information about the majority of the users in that cluster segment.\n",
    "\n",
    "I've used these charts to review my previously stated insights, but follow the same for you to explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "05016fec7a1e1efd7cf8abc2670ebebe86b31a35",
    "scrolled": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_data = ['Cluster 0', 'Cluster 1','Cluster 2','Cluster 3','Cluster 4', 'Cluster 5', 'Cluster 6']\n",
    "colors = ['rgba(93, 164, 214, 0.5)', 'rgba(255, 144, 14, 0.5)', 'rgba(44, 160, 101, 0.5)', 'rgba(255, 65, 54, 0.5)', \n",
    "          'rgba(22, 80, 57, 0.5)', 'rgba(127, 65, 14, 0.5)', 'rgba(207, 114, 255, 0.5)', 'rgba(127, 96, 0, 0.5)']\n",
    "cutoff_quantile = 95\n",
    "\n",
    "for n_clusters in range(3,K_best+1,2):\n",
    "    cl = 'clusters_' + str(n_clusters)\n",
    "    for fild in range(0, 3):\n",
    "        field_to_plot = features[fild]        \n",
    "        y_data = list()\n",
    "        ymax = 0\n",
    "        for i in np.arange(0,n_clusters):\n",
    "            y0 = customer_history_df[customer_history_df[cl]==i][field_to_plot].values\n",
    "            y0 = y0[y0<np.percentile(y0, cutoff_quantile)]\n",
    "            if ymax < max(y0): ymax = max(y0)\n",
    "            y_data.insert(i, y0)\n",
    "\n",
    "        traces = []\n",
    "\n",
    "        for xd, yd, cls in zip(x_data[:n_clusters], y_data, colors[:n_clusters]):\n",
    "                traces.append(go.Box(y=yd, name=xd, boxpoints=False, jitter=0.5, whiskerwidth=0.2, fillcolor=cls,\n",
    "                    marker=dict( size=1, ),\n",
    "                    line=dict(width=1),\n",
    "                ))\n",
    "\n",
    "        layout = go.Layout(\n",
    "            title='Difference in {} with {} Clusters and {:1.2f} Score'.\\\n",
    "            format(field_to_plot, n_clusters, cluster_centers[n_clusters]['silhouette_score']),\n",
    "            yaxis=dict( autorange=True, showgrid=True, zeroline=True,\n",
    "                dtick = int(ymax/10),\n",
    "                gridcolor='black', gridwidth=0.1, zerolinecolor='rgb(255, 255, 255)', zerolinewidth=2, ),\n",
    "            margin=dict(l=40, r=30, b=50, t=50, ),\n",
    "            paper_bgcolor='white',\n",
    "            plot_bgcolor='white',\n",
    "            showlegend=False\n",
    "        )\n",
    "\n",
    "        fig = go.Figure(data=traces, layout=layout)\n",
    "        py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d7cc3deaa77648e0d70fccac2a97ef788e7176a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "items = list(cs_df.Description.unique())\n",
    "grouped = cs_df.groupby('InvoiceNo')\n",
    "transaction_level = grouped.aggregate(lambda x: tuple(x)).reset_index()[['InvoiceNo','Description']]\n",
    "transaction_dict = {item:0 for item in items}\n",
    "output_dict = dict()\n",
    "temp = dict()\n",
    "for rec in transaction_level.to_dict('records'):\n",
    "    invoice_num = rec['InvoiceNo']\n",
    "    items_list = rec['Description']\n",
    "    transaction_dict = {item:0 for item in items}\n",
    "    transaction_dict.update({item:1 for item in items if item in items_list})\n",
    "    temp.update({invoice_num:transaction_dict})\n",
    "\n",
    "new = [v for k,v in temp.items()]\n",
    "transaction_df = pd.DataFrame(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5fce855898ea5bff4137b1d1d43eca606f646b7f"
   },
   "source": [
    "#### Prune Dataset for frequently purchased items\n",
    "We saw in the earlier on EDA how only a handful of items are responsible for bulk of our sales so we want to prune our dataset to reflect this information. For this we have created a function prune_dataset below, which will help us reduce the size of our dataset based on our requirements. The function can be used for performing three types of pruning:\n",
    "- Pruning based on percentage of total sales: The parameter total_sales_perc will help us select the number of items that will explain the required percentage of sales. The default value is 50% or 0.5.\n",
    "- Pruning based on ranks of items: Another way to perform the pruning is to specify the starting and the ending rank of the items for which we want to prune our dataset.\n",
    "- Pruning based on list of features passed to the parameter TopCols. \n",
    "\n",
    "By default, we will only look for transactions which have at least two items, as transactions with only one item are counter to the whole concept of association rule-mining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2456a783ea6a1475e1e6faa3f911de86e4a72f6f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prune_dataset(input_df, length_trans = 2, total_sales_perc = 0.5, \n",
    "                  start_item = None, end_item = None, TopCols = None):\n",
    "    if 'total_items' in input_df.columns:\n",
    "        del(input_df['total_items'])\n",
    "    item_count = input_df.sum().sort_values(ascending = False).reset_index()\n",
    "    total_items = sum(input_df.sum().sort_values(ascending = False))\n",
    "    item_count.rename(columns={item_count.columns[0]:'item_name',\n",
    "                               item_count.columns[1]:'item_count'}, inplace=True)\n",
    "    if TopCols: \n",
    "        input_df['total_items'] = input_df[TopCols].sum(axis = 1)\n",
    "        input_df = input_df[input_df.total_items >= length_trans]\n",
    "        del(input_df['total_items'])\n",
    "        return input_df[TopCols], item_count[item_count.item_name.isin(TopCols)]\n",
    "    elif end_item > start_item:\n",
    "        selected_items = list(item_count[start_item:end_item].item_name)\n",
    "        input_df['total_items'] = input_df[selected_items].sum(axis = 1)\n",
    "        input_df = input_df[input_df.total_items >= length_trans]\n",
    "        del(input_df['total_items'])\n",
    "        return input_df[selected_items],item_count[start_item:end_item]\n",
    "    else:\n",
    "        item_count['item_perc'] = item_count['item_count']/total_items\n",
    "        item_count['total_perc'] = item_count.item_perc.cumsum()\n",
    "        selected_items = list(item_count[item_count.total_perc < total_sales_perc].item_name)\n",
    "        input_df['total_items'] = input_df[selected_items].sum(axis = 1)\n",
    "        input_df = input_df[input_df.total_items >= length_trans]\n",
    "        del(input_df['total_items'])\n",
    "        return input_df[selected_items], item_count[item_count.total_perc < total_sales_perc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5e53606adfe1f79bf9165d202f9d1fb8dd958bad"
   },
   "source": [
    "We use the second option of pruning, by the Top 15th products in sales events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1f01741236474dcc8dbe08460837dc91ab6d9fc3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_df, item_counts = prune_dataset(input_df=transaction_df, length_trans=2,start_item = 0, end_item = 15)\n",
    "print('Total of Sales Amount by the Top 15 Products in Sales Events (Invoice): {:.2f}'.format(AmoutSum[Top15ev].sum()))\n",
    "print('Number of Sales Events:', output_df.shape[0])\n",
    "print('Number of Products:', output_df.shape[1])\n",
    "\n",
    "item_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3275361ff6eb4fcd92ddba56944fc6ae29e5d7b7"
   },
   "source": [
    "So we find out that we have 15 items responsible for 8,73% of sales amount and close to 5% of the events result in 4.664 transactions that have those items along with other items. The next step is to convert this selected data into the required table data structure.\n",
    "\n",
    "#### Association Rule Mining with FP Growth\n",
    "\n",
    "##### Orange Table Data Structure\n",
    "Since we are using the Orage framework we still have to convert it to the Table data structure by providing the metadata about our columns. We need to define the domain for each of our variables. The domain means the possible set of values that each of our variables can use. This information will be stored as metadata and will be used in later transformation of the data. As our columns are only having binary values,we can easily create the domain by using this information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "61f80e7494f294ff19465ca7ffab764037384f83",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_assoc_rules = output_df\n",
    "# Defined the data domain by specifying each variable as a DiscreteVariable having values as (0, 1)\n",
    "domain_transac = Domain([DiscreteVariable.make(name=item,values=['0', '1']) \\\n",
    "                         for item in input_assoc_rules.columns])\n",
    "\n",
    "# Then using this domain, we created our Table structure for our data\n",
    "data_tran = Orange.data.Table.from_numpy(domain=domain_transac,  \n",
    "                                         X=input_assoc_rules.as_matrix(),Y= None)\n",
    "\n",
    "# Coding our input so that the entire domain is represented as binary variables\n",
    "data_tran_en, mapping = OneHot.encode(data_tran, include_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5a1f0a9163a3734e307102f622ebd065d59a7795"
   },
   "source": [
    "##### Creating our rules\n",
    "We need to specify two pieces of information for generating our rules: support and confidence. An important piece of information is to start with a higher support, as lower support will mean a higher number of frequent itemsets and hence a longer execution time. We will specify a min support of 0.01 and see the number of frequent itemsets that we get before we specify confidence and generate our rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6118dabc2f2dd2e4452ac2e7212e69fb69bd853b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "support = 0.01\n",
    "print(\"num of required transactions = \", int(input_assoc_rules.shape[0]*support))\n",
    "num_trans = input_assoc_rules.shape[0]*support\n",
    "itemsets = dict(frequent_itemsets(data_tran_en, support))\n",
    "print('Items Set Size:', len(itemsets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2508d0489a73ee71872f7b1807b07cbd0238c764"
   },
   "source": [
    "So we get a whopping 663.273 itemsets for only 15 itens and a support of only 1%! This will increase exponentially if we decrease the support or if we increase the number of items in our dataset. The next step is specifying a confidence value and generating our rules. The following code snippet will perform rule generation and decoding of rules, and then compile it all in a neat dataframe that we can use for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "897100a4d86fe4a0608124f0051992e8bbef659d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "confidence = 0.6\n",
    "rules_df = pd.DataFrame()\n",
    "if len(itemsets) < 1000000: \n",
    "    rules = [(P, Q, supp, conf)\n",
    "    for P, Q, supp, conf in association_rules(itemsets, confidence)\n",
    "       if len(Q) == 1 ]\n",
    "\n",
    "    names = {item: '{}={}'.format(var.name, val)\n",
    "        for item, var, val in OneHot.decode(mapping, data_tran, mapping)}\n",
    "    \n",
    "    eligible_ante = [v for k,v in names.items() if v.endswith(\"1\")]\n",
    "    \n",
    "    N = input_assoc_rules.shape[0]\n",
    "    \n",
    "    rule_stats = list(rules_stats(rules, itemsets, N))\n",
    "    \n",
    "    rule_list_df = []\n",
    "    for ex_rule_frm_rule_stat in rule_stats:\n",
    "        ante = ex_rule_frm_rule_stat[0]            \n",
    "        cons = ex_rule_frm_rule_stat[1]\n",
    "        named_cons = names[next(iter(cons))]\n",
    "        if named_cons in eligible_ante:\n",
    "            rule_lhs = [names[i][:-2] for i in ante if names[i] in eligible_ante]\n",
    "            ante_rule = ', '.join(rule_lhs)\n",
    "            if ante_rule and len(rule_lhs)>1 :\n",
    "                rule_dict = {'support' : ex_rule_frm_rule_stat[2],\n",
    "                             'confidence' : ex_rule_frm_rule_stat[3],\n",
    "                             'coverage' : ex_rule_frm_rule_stat[4],\n",
    "                             'strength' : ex_rule_frm_rule_stat[5],\n",
    "                             'lift' : ex_rule_frm_rule_stat[6],\n",
    "                             'leverage' : ex_rule_frm_rule_stat[7],\n",
    "                             'antecedent': ante_rule,\n",
    "                             'consequent':named_cons[:-2] }\n",
    "                rule_list_df.append(rule_dict)\n",
    "    rules_df = pd.DataFrame(rule_list_df)\n",
    "    print(\"Raw rules data frame of {} rules generated\".format(rules_df.shape[0]))\n",
    "    if not rules_df.empty:\n",
    "        pruned_rules_df = rules_df.groupby(['antecedent','consequent']).max().reset_index()\n",
    "    else:\n",
    "        print(\"Unable to generate any rule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "872a78070cdf48ad1060d6c9063ed05bb67fc71a"
   },
   "source": [
    "##### Explore The Association Rule Created\n",
    "\n",
    "Let's see what we get in the first 5 rules with highest confidence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f37df3eb00f2a97f871bfadf3a0d4621202ee3fe",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dw = pd.options.display.max_colwidth\n",
    "pd.options.display.max_colwidth = 100\n",
    "(rules_df[['consequent', 'antecedent', 'support','confidence','lift']].\\\n",
    " groupby(['consequent', 'antecedent']).first()\n",
    "                                      .reset_index()\n",
    "                                      .sort_values(['confidence', 'support', 'lift'],\n",
    "                                                   ascending=False)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "889c9bdbea28dd0f9927530cb480004680cc7ec1"
   },
   "source": [
    "Now, the first 5 higest support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4802211adb10ff2588ff590ab7b6c58322393370",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "(rules_df[['consequent', 'antecedent', 'support','confidence','lift']].\\\n",
    " groupby(['consequent', 'antecedent']).first()\n",
    "                                      .reset_index()\n",
    "                                      .sort_values(['support', 'confidence', 'lift'],\n",
    "                                                   ascending=False)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8c98897f27d40b27be85e1e1f73d2921bcfd80a3"
   },
   "source": [
    "Typically, a lift value of 1 indicates that the probability of occurrence of the antecedent and consequent together are independent of each other. Hence, the idea is to look for rules having a lift much greater than 1.  So, let's see how much rules has lift greater than 1, equal 1 and less than one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ecfd52ebf26177cf644d416001099ed19ed656c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rules_df.lift.apply(lambda x: 'Greater Than One' if x > 1 else 'One' \\\n",
    "                           if x == 0 else 'Less Than One').value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9cc3ad886cd4f4b6cb2519269794b45043ada0f1"
   },
   "source": [
    "So all rules are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "edcd09bf5f74555411f54c89536b12b205c0a4b4",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2039c7200efd0dc0c47d01a26d02701cd7b538bc"
   },
   "source": [
    "#### Conclusion\n",
    "\n",
    "As we have seen, the generation of rules is a very simple process, but very computationally expensive, since it grows exponentially with the increase of the set of items.\n",
    "\n",
    "Overall, we seek the proper balance between support and confidence leading to a reasonable number of strong rules. \n",
    "\n",
    "In the other hand, if the goal is to identify rare but with high confidence patterns, we should proceed as previously, by establishing a low level of support and a higher level of confidence, which leads to a large number of rules.\n",
    "\n",
    "With this in mind, the rules with low support and high confidence would then be our target for further study and than outlining of strategies to raise cross selling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
